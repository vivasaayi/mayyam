use sea_orm::{DatabaseConnection, DbBackend, Statement};
use chrono::{DateTime, Utc};
use std::collections::HashMap;
use tracing;

use crate::models::database::*;
use crate::errors::AppError;
use crate::config::Config;
use crate::utils::database_ext::DatabaseConnectionExt;
use serde_json;

pub struct DatabaseService {
    config: Config,
}

impl DatabaseService {
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    pub async fn analyze_database(&self, conn: &DatabaseConnection) -> Result<DatabaseAnalysis, AppError> {
        let mut analysis = DatabaseAnalysis {
            issues: Vec::new(),
            query_stats: self.get_query_statistics(conn).await?,
            performance_metrics: self.get_performance_metrics(conn).await?,
            cost_analysis: self.analyze_costs(conn).await?,
        };

        // Analyze and collect issues
        self.analyze_performance_issues(conn, &mut analysis.issues).await?;
        self.analyze_storage_issues(conn, &mut analysis.issues).await?;
        self.analyze_security_issues(conn, &mut analysis.issues).await?;
        self.analyze_configuration_issues(conn, &mut analysis.issues).await?;

        Ok(analysis)
    }

    async fn get_query_statistics(&self, conn: &DatabaseConnection) -> Result<QueryStatistics, AppError> {
        // For PostgreSQL
        let stats = conn.query_one(Statement::from_string(
            DbBackend::Postgres,
            r#"
            WITH QueryStats AS (
                SELECT query,
                    calls as execution_count,
                    total_time / 1000 as total_time_ms,
                    mean_time / 1000 as avg_time_ms,
                    rows as total_rows,
                    shared_blks_hit + shared_blks_read as total_blocks,
                    max(last_call) as last_execution
                FROM pg_stat_statements
                WHERE dbid = (SELECT oid FROM pg_database WHERE datname = current_database())
            )
            SELECT 
                (SELECT COUNT(*) FROM QueryStats) as total_queries,
                (SELECT COUNT(*) FROM QueryStats WHERE avg_time_ms > 1000) as slow_queries,
                (SELECT AVG(avg_time_ms) FROM QueryStats) as avg_query_time
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let slow_queries = self.get_slow_queries(conn).await?;
        let frequent_queries = self.get_frequent_queries(conn).await?;

        Ok(QueryStatistics {
            total_queries: stats.try_get::<i64, _>("total_queries")?,
            slow_queries: stats.try_get::<i64, _>("slow_queries")?,
            avg_query_time_ms: stats.try_get::<f64, _>("avg_query_time")?,
            top_slow_queries: slow_queries,
            frequent_queries,
        })
    }

    async fn get_performance_metrics(&self, conn: &DatabaseConnection) -> Result<PerformanceMetrics, AppError> {
        let stats = conn.query_one(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') as max_conn,
                (SELECT count(*) FROM pg_stat_activity) as current_conn,
                (SELECT count(*) FROM pg_stat_activity WHERE state = 'idle') as idle_conn,
                (SELECT sum(blks_hit)::float / (sum(blks_hit) + sum(blks_read)) FROM pg_stat_database) as buffer_ratio,
                (SELECT extract(epoch from now() - pg_postmaster_start_time())) as uptime,
                (SELECT count(*) FROM pg_locks WHERE granted = false) as blocked_queries
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let table_stats = self.get_table_statistics(conn).await?;
        let index_stats = self.get_index_statistics(conn).await?;

        Ok(PerformanceMetrics {
            connection_count: stats.try_get::<i32, _>("current_conn")?,
            active_sessions: stats.try_get::<i32, _>("current_conn")? - stats.try_get::<i32, _>("idle_conn")?,
            idle_sessions: stats.try_get::<i32, _>("idle_conn")?,
            buffer_hit_ratio: stats.try_get::<f64, _>("buffer_ratio")?,
            cache_hit_ratio: 0.0, // Needs additional calculation
            deadlocks: 0, // Need to get from pg_stat_database
            blocked_queries: stats.try_get::<i64, _>("blocked_queries")?,
            table_stats,
            index_stats,
        })
    }

    async fn analyze_costs(&self, conn: &DatabaseConnection) -> Result<CostAnalysis, AppError> {
        let storage_metrics = self.get_storage_metrics(conn).await?;
        let compute_metrics = self.get_compute_metrics(conn).await?;
        
        let storage_cost = ResourceCost {
            current_usage: storage_metrics.total_bytes as f64,
            unit: "GB".to_string(),
            cost_per_unit: 0.10, // Example cost per GB
            total_cost: (storage_metrics.total_bytes as f64 / 1024.0 / 1024.0 / 1024.0) * 0.10,
            trending: if storage_metrics.growth_rate > 0.0 {
                TrendDirection::Increasing
            } else if storage_metrics.growth_rate < 0.0 {
                TrendDirection::Decreasing
            } else {
                TrendDirection::Stable
            },
        };

        // Calculate other costs...
        Ok(CostAnalysis {
            storage_cost,
            compute_cost: ResourceCost {
                current_usage: compute_metrics.cpu_usage,
                unit: "vCPU hours".to_string(),
                cost_per_unit: 0.05,
                total_cost: compute_metrics.cpu_usage * 0.05,
                trending: TrendDirection::Stable,
            },
            network_cost: ResourceCost {
                current_usage: 0.0,
                unit: "GB".to_string(),
                cost_per_unit: 0.09,
                total_cost: 0.0,
                trending: TrendDirection::Stable,
            },
            backup_cost: ResourceCost {
                current_usage: storage_metrics.total_bytes as f64,
                unit: "GB".to_string(),
                cost_per_unit: 0.05,
                total_cost: (storage_metrics.total_bytes as f64 / 1024.0 / 1024.0 / 1024.0) * 0.05,
                trending: TrendDirection::Stable,
            },
            total_monthly_cost: 0.0, // Calculate total
            cost_recommendations: self.generate_cost_recommendations(conn).await?,
        })
    }

    async fn analyze_performance_issues(&self, conn: &DatabaseConnection, issues: &mut Vec<DatabaseIssue>) -> Result<(), AppError> {
        // Check for missing indexes on frequently queried columns
        let missing_indexes = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT schemaname, tablename, rows_per_scan
            FROM (
                SELECT schemaname, tablename,
                    CASE WHEN seq_scan > 0 THEN (seq_tup_read::float / seq_scan) ELSE 0 END as rows_per_scan
                FROM pg_stat_user_tables
                WHERE seq_scan > 0
            ) t
            WHERE rows_per_scan > 1000
            ORDER BY rows_per_scan DESC
            LIMIT 10
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        for row in missing_indexes {
            issues.push(DatabaseIssue {
                severity: IssueSeverity::High,
                category: IssueCategory::Performance,
                title: format!("Missing index on frequently scanned table {}", row.try_get::<String, _>("tablename")?),
                description: format!(
                    "Table {} is frequently scanned with {} rows per scan on average",
                    row.try_get::<String, _>("tablename")?,
                    row.try_get::<f64, _>("rows_per_scan")?
                ),
                recommendation: "Consider adding an index on the commonly queried columns".to_string(),
                affected_objects: vec![format!(
                    "{}.{}",
                    row.try_get::<String, _>("schemaname")?,
                    row.try_get::<String, _>("tablename")?
                )],
            });
        }

        // Add more performance issue checks...
        Ok(())
    }

    async fn get_table_statistics(&self, conn: &DatabaseConnection) -> Result<Vec<TableStats>, AppError> {
        let stats = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                relname as table_name,
                pg_table_size(c.oid) as table_size,
                n_live_tup as row_count,
                n_dead_tup as dead_rows,
                seq_scan,
                idx_scan,
                last_vacuum,
                last_analyze
            FROM pg_stat_user_tables s
            JOIN pg_class c ON s.relid = c.oid
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let mut table_stats = Vec::new();
        for row in stats {
            table_stats.push(TableStats {
                name: row.try_get::<String, _>("table_name")?,
                size_bytes: row.try_get::<i64, _>("table_size")?,
                total_rows: row.try_get::<i64, _>("row_count")?,
                sequential_scans: row.try_get::<i64, _>("seq_scan")?,
                index_scans: row.try_get::<i64, _>("idx_scan")?,
                live_row_count: row.try_get::<i64, _>("row_count")?,
                dead_row_count: row.try_get::<i64, _>("dead_rows")?,
                last_vacuum: row.try_get::<Option<DateTime<Utc>>, _>("last_vacuum")?,
                last_analyze: row.try_get::<Option<DateTime<Utc>>, _>("last_analyze")?,
            });
        }

        Ok(table_stats)
    }

    async fn get_index_statistics(&self, conn: &DatabaseConnection) -> Result<Vec<IndexStats>, AppError> {
        let stats = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                i.relname as index_name,
                t.relname as table_name,
                pg_relation_size(i.oid) as index_size,
                idx_scan,
                idx_tup_read,
                indisunique,
                indisprimary,
                last_used = NULL AND idx_scan = 0 as is_unused,
                last_used
            FROM pg_stat_user_indexes s
            JOIN pg_class i ON s.indexrelid = i.oid
            JOIN pg_class t ON s.relid = t.oid
            JOIN pg_index idx ON s.indexrelid = idx.indexrelid
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let mut index_stats = Vec::new();
        for row in stats {
            index_stats.push(IndexStats {
                name: row.try_get::<String, _>("index_name")?,
                table_name: row.try_get::<String, _>("table_name")?,
                size_bytes: row.try_get::<i64, _>("index_size")?,
                is_unique: row.try_get::<bool, _>("indisunique")?,
                is_primary: row.try_get::<bool, _>("indisprimary")?,
                index_scans: row.try_get::<i64, _>("idx_scan")?,
                rows_fetched: row.try_get::<i64, _>("idx_tup_read")?,
                unused_since: if row.try_get::<bool, _>("is_unused")? {
                    Some(row.try_get::<DateTime<Utc>, _>("last_used")?)
                } else {
                    None
                },
            });
        }

        Ok(index_stats)
    }

    async fn generate_cost_recommendations(&self, conn: &DatabaseConnection) -> Result<Vec<CostRecommendation>, AppError> {
        let mut recommendations = Vec::new();

        // Check for unused indexes
        let unused_indexes = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                schemaname, tablename, indexname,
                pg_relation_size(i.indexrelid) as index_size
            FROM pg_stat_user_indexes i
            JOIN pg_index idx ON i.indexrelid = idx.indexrelid
            WHERE idx_scan = 0 
                AND NOT indisprimary 
                AND NOT indisunique
                AND NOT EXISTS (
                    SELECT 1 FROM pg_constraint c 
                    WHERE c.conindid = i.indexrelid
                )
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        for row in unused_indexes {
            let index_size = row.try_get::<i64, _>("index_size")?;
            recommendations.push(CostRecommendation {
                title: format!("Remove unused index {}", row.try_get::<String, _>("indexname")?),
                description: format!(
                    "Index {} on table {}.{} is never used and consumes {} MB of storage",
                    row.try_get::<String, _>("indexname")?,
                    row.try_get::<String, _>("schemaname")?,
                    row.try_get::<String, _>("tablename")?,
                    index_size / 1024 / 1024
                ),
                estimated_savings: (index_size as f64 / 1024.0 / 1024.0 / 1024.0) * 0.10, // Assuming $0.10 per GB
                implementation_effort: "Low".to_string(),
                priority: "Medium".to_string(),
            });
        }

        // Add more cost optimization recommendations...
        Ok(recommendations)
    }

    async fn analyze_storage_issues(&self, conn: &DatabaseConnection, issues: &mut Vec<DatabaseIssue>) -> Result<(), AppError> {
        // Check for tables with high bloat
        let bloated_tables = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT
                schemaname, tablename,
                pg_table_size(schemaname || '.' || tablename) as table_size,
                n_dead_tup::float / n_live_tup as dead_ratio
            FROM pg_stat_user_tables
            WHERE n_live_tup > 0 AND n_dead_tup::float / n_live_tup > 0.2
            ORDER BY dead_ratio DESC
            LIMIT 10
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        for row in bloated_tables {
            issues.push(DatabaseIssue {
                severity: IssueSeverity::Medium,
                category: IssueCategory::Storage,
                title: format!("Table {} has high dead tuple ratio", row.try_get::<String, _>("tablename")?),
                description: format!(
                    "Table {}.{} has {}% dead tuples, wasting disk space",
                    row.try_get::<String, _>("schemaname")?,
                    row.try_get::<String, _>("tablename")?,
                    (row.try_get::<f64, _>("dead_ratio")? * 100.0).round()
                ),
                recommendation: "Run VACUUM FULL to reclaim space".to_string(),
                affected_objects: vec![format!(
                    "{}.{}",
                    row.try_get::<String, _>("schemaname")?,
                    row.try_get::<String, _>("tablename")?
                )],
            });
        }

        // Add more storage issues checks
        Ok(())
    }

    async fn analyze_security_issues(&self, conn: &DatabaseConnection, issues: &mut Vec<DatabaseIssue>) -> Result<(), AppError> {
        // Check for excessive privileges
        let excessive_privileges = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                grantee, table_schema, table_name, privilege_type
            FROM information_schema.table_privileges
            WHERE grantee = 'public' AND privilege_type IN ('INSERT', 'UPDATE', 'DELETE', 'TRUNCATE')
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        for row in excessive_privileges {
            issues.push(DatabaseIssue {
                severity: IssueSeverity::High,
                category: IssueCategory::Security,
                title: format!("Excessive {} privilege for PUBLIC on {}", 
                    row.try_get::<String, _>("privilege_type")?,
                    row.try_get::<String, _>("table_name")?),
                description: format!(
                    "Table {}.{} grants {} privilege to PUBLIC user",
                    row.try_get::<String, _>("table_schema")?,
                    row.try_get::<String, _>("table_name")?,
                    row.try_get::<String, _>("privilege_type")?
                ),
                recommendation: "Revoke excessive permissions and grant only to specific roles".to_string(),
                affected_objects: vec![format!(
                    "{}.{}",
                    row.try_get::<String, _>("table_schema")?,
                    row.try_get::<String, _>("table_name")?
                )],
            });
        }

        // Add more security issue checks
        Ok(())
    }

    async fn analyze_configuration_issues(&self, conn: &DatabaseConnection, issues: &mut Vec<DatabaseIssue>) -> Result<(), AppError> {
        // Check for suboptimal configuration settings
        let config_settings = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                name, setting, unit, context
            FROM pg_settings
            WHERE name IN (
                'shared_buffers', 'work_mem', 'maintenance_work_mem',
                'effective_cache_size', 'max_connections'
            )
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        // Very simple check for shared_buffers
        for row in config_settings {
            let setting_name = row.try_get::<String, _>("name")?;
            if setting_name == "shared_buffers" {
                let value = row.try_get::<String, _>("setting")?.parse::<i64>().unwrap_or(0);
                let unit = row.try_get::<String, _>("unit")?;
                
                // Simple check - should be at least 128MB for production
                if value < 128 && unit == "MB" {
                    issues.push(DatabaseIssue {
                        severity: IssueSeverity::Medium,
                        category: IssueCategory::Configuration,
                        title: "Shared buffers setting is too low".to_string(),
                        description: format!("shared_buffers is set to {}MB, which may be too low for optimal performance", value),
                        recommendation: "Consider increasing shared_buffers to at least 25% of system memory".to_string(),
                        affected_objects: vec!["postgresql.conf".to_string()],
                    });
                }
            }
        }

        // Add more configuration issue checks
        Ok(())
    }

    async fn get_slow_queries(&self, conn: &DatabaseConnection) -> Result<Vec<SlowQuery>, AppError> {
        // Query for slow queries from pg_stat_statements
        let slow_queries = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                query, 
                mean_time / 1000 as avg_execution_time_ms,
                calls as execution_count,
                max(last_call) as last_execution
            FROM pg_stat_statements
            WHERE mean_time / 1000 > 1000 -- queries taking more than 1 second
            ORDER BY mean_time DESC
            LIMIT 10
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let mut result = Vec::new();
        for row in slow_queries {
            result.push(SlowQuery {
                query: row.try_get::<String, _>("query")?,
                avg_execution_time_ms: row.try_get::<f64, _>("avg_execution_time_ms")?,
                execution_count: row.try_get::<i64, _>("execution_count")?,
                last_execution: row.try_get::<DateTime<Utc>, _>("last_execution")?,
                query_plan: None, // Would require EXPLAIN ANALYZE which we'll omit here
            });
        }

        Ok(result)
    }

    async fn get_frequent_queries(&self, conn: &DatabaseConnection) -> Result<Vec<FrequentQuery>, AppError> {
        // Query for frequently executed queries
        let frequent_queries = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                query,
                calls as execution_count,
                mean_time / 1000 as avg_execution_time_ms,
                total_time / 1000 as total_time_ms
            FROM pg_stat_statements
            ORDER BY calls DESC
            LIMIT 10
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let mut result = Vec::new();
        for row in frequent_queries {
            result.push(FrequentQuery {
                query: row.try_get::<String, _>("query")?,
                execution_count: row.try_get::<i64, _>("execution_count")?,
                avg_execution_time_ms: row.try_get::<f64, _>("avg_execution_time_ms")?,
                total_time_ms: row.try_get::<f64, _>("total_time_ms")?,
            });
        }

        Ok(result)
    }

    async fn get_storage_metrics(&self, conn: &DatabaseConnection) -> Result<StorageMetrics, AppError> {
        // Query for database size and growth stats
        let storage_stats = conn.query_one(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT
                pg_database_size(current_database()) as database_size,
                COALESCE(
                    (SELECT sum(pg_relation_size(c.oid))
                     FROM pg_class c
                     JOIN pg_namespace n ON c.relnamespace = n.oid
                     WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
                    ), 0) as user_data_size,
                COALESCE(
                    (SELECT sum(pg_total_relation_size(c.oid) - pg_relation_size(c.oid))
                     FROM pg_class c
                     JOIN pg_namespace n ON c.relnamespace = n.oid
                     WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
                    ), 0) as index_size
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        // In a real implementation, you'd track historical data to calculate growth rate
        // For now, we'll just use a dummy value
        let growth_rate = 0.02; // 2% growth per day, for example

        Ok(StorageMetrics {
            total_bytes: storage_stats.try_get::<i64, _>("database_size")?,
            user_data_bytes: storage_stats.try_get::<i64, _>("user_data_size")?,
            index_bytes: storage_stats.try_get::<i64, _>("index_size")?,
            free_space_bytes: 0, // Would need to query file system
            growth_rate, // Hard-coded for now
            estimate_days_until_full: if growth_rate > 0.0 { Some(100.0 / (growth_rate * 100.0)) } else { None },
            top_tables_by_size: self.get_top_tables_by_size(conn).await?,
        })
    }

    async fn get_top_tables_by_size(&self, conn: &DatabaseConnection) -> Result<HashMap<String, i64>, AppError> {
        let top_tables = conn.query_all(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT 
                relname as table_name,
                pg_total_relation_size(c.oid) as total_size
            FROM pg_class c
            JOIN pg_namespace n ON c.relnamespace = n.oid
            WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
              AND c.relkind = 'r'
            ORDER BY total_size DESC
            LIMIT 10
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        let mut result = HashMap::new();
        for row in top_tables {
            result.insert(
                row.try_get::<String, _>("table_name")?,
                row.try_get::<i64, _>("total_size")?,
            );
        }

        Ok(result)
    }

    async fn get_compute_metrics(&self, conn: &DatabaseConnection) -> Result<ComputeMetrics, AppError> {
        // Query for CPU usage and related stats
        let compute_stats = conn.query_one(Statement::from_string(
            DbBackend::Postgres,
            r#"
            SELECT
                (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections,
                (SELECT extract(epoch from now() - pg_postmaster_start_time())) as uptime_seconds
            "#,
        ))
        .await
        .map_err(AppError::Database)?;

        // CPU usage calculation is complex and requires OS stats
        // This is a simplified placeholder implementation
        let active_connections = compute_stats.try_get::<i64, _>("active_connections")?;
        let uptime_seconds = compute_stats.try_get::<f64, _>("uptime_seconds")?;
        
        // Very rough estimate - in real life would use OS metrics
        let cpu_usage = active_connections as f64 * 0.1; // Assume each connection uses 10% CPU

        Ok(ComputeMetrics {
            cpu_usage,
            memory_usage_bytes: 0, // Would need OS metrics
            active_connections: active_connections as i32,
            uptime_seconds,
        })
    }

    // Execute a database query on the specified connection
    pub async fn execute_query(&self, 
        conn_model: &crate::models::database::Model,
        query: &str, 
        params: Option<&serde_json::Value>
    ) -> Result<DatabaseQueryResponse, AppError> {
        // Implement connection logic based on connection type
        let start_time = Utc::now();
        let (columns, rows) = match conn_model.connection_type.as_str() {
            "postgres" => self.execute_postgres_query(conn_model, query, params).await?,
            "mysql" => self.execute_mysql_query(conn_model, query, params).await?,
            "redis" => self.execute_redis_query(conn_model, query, params).await?,
            "opensearch" => self.execute_opensearch_query(conn_model, query, params).await?,
            _ => return Err(AppError::BadRequest(format!("Unsupported database type: {}", conn_model.connection_type)))
        };
        
        let execution_time = (Utc::now() - start_time).num_milliseconds() as u64;
        let row_count = rows.len();
        
        Ok(DatabaseQueryResponse {
            columns,
            rows,
            execution_time_ms: execution_time,
            row_count,
            query_plan: None,
        })
    }
    
    // Execute a query with EXPLAIN plan
    pub async fn execute_query_with_explain(&self, 
        conn_model: &crate::models::database::Model,
        query: &str, 
        params: Option<&serde_json::Value>
    ) -> Result<DatabaseQueryResponse, AppError> {
        // First get regular query results
        let mut response = self.execute_query(conn_model, query, params).await?;
        
        // Only PostgreSQL and MySQL support EXPLAIN
        match conn_model.connection_type.as_str() {
            "postgres" => {
                // Execute EXPLAIN command
                let explain_query = format!("EXPLAIN (FORMAT JSON) {}", query);
                let (_columns, rows) = self.execute_postgres_query(conn_model, &explain_query, params).await?;
                
                if !rows.is_empty() && rows[0].is_object() {
                    // In a real implementation, you would parse the JSON plan into your QueryPlan structure
                    // Here we're just creating a simplified example plan
                    response.query_plan = Some(QueryPlan {
                        plan_type: "PostgreSQL".to_string(),
                        total_cost: 0.0, // Would be extracted from the actual plan
                        planning_time_ms: 0.0,
                        execution_time_ms: response.execution_time_ms as f64,
                        nodes: vec![
                            QueryPlanNode {
                                node_type: "Root".to_string(),
                                actual_rows: response.row_count as i64,
                                plan_rows: response.row_count as i64,
                                actual_time_ms: response.execution_time_ms as f64,
                                total_cost: 0.0,
                                description: "Query Plan Root".to_string(),
                                children: Vec::new(),
                            }
                        ],
                    });
                }
            },
            "mysql" => {
                // For MySQL, we'd use a different EXPLAIN syntax
                // But for now, just use the same mock plan for simplicity
                response.query_plan = Some(QueryPlan {
                    plan_type: "MySQL".to_string(),
                    total_cost: 0.0,
                    planning_time_ms: 0.0,
                    execution_time_ms: response.execution_time_ms as f64,
                    nodes: vec![
                        QueryPlanNode {
                            node_type: "Root".to_string(),
                            actual_rows: response.row_count as i64,
                            plan_rows: response.row_count as i64,
                            actual_time_ms: response.execution_time_ms as f64,
                            total_cost: 0.0,
                            description: "MySQL Query Plan".to_string(),
                            children: Vec::new(),
                        }
                    ],
                });
            },
            _ => {
                // Other database types don't support EXPLAIN
                // Just return the query results as is
            }
        }
        
        Ok(response)
    }
    
    // Helper functions for specific database types
    async fn execute_postgres_query(&self, 
        conn_model: &crate::models::database::Model,
        query: &str,
        params: Option<&serde_json::Value>
    ) -> Result<(Vec<String>, Vec<serde_json::Value>), AppError> {
        // In a real implementation, we would:
        // 1. Build a connection string using conn_model details
        // 2. Establish a connection to PostgreSQL
        // 3. Execute the query with parameters
        // 4. Parse the results
        
        // Log what would happen in a real implementation
        tracing::debug!(
            "Would execute PostgreSQL query on {}:{}/{}: {}",
            conn_model.host, 
            conn_model.port, 
            conn_model.database_name.as_deref().unwrap_or(""),
            query
        );
        
        if let Some(p) = params {
            tracing::debug!("With parameters: {}", p);
        }
        
        // For now, we'll return mock data
        let columns = vec!["id".to_string(), "name".to_string(), "value".to_string()];
        let mut rows = Vec::new();
        
        // Create some mock data
        for i in 1..5 {
            let row = serde_json::json!({
                "id": i,
                "name": format!("Item {}", i),
                "value": i * 10
            });
            rows.push(row);
        }
        
        Ok((columns, rows))
    }
    
    async fn execute_mysql_query(&self, 
        conn_model: &crate::models::database::Model,
        query: &str,
        params: Option<&serde_json::Value>
    ) -> Result<(Vec<String>, Vec<serde_json::Value>), AppError> {
        // Log what would happen in a real implementation
        tracing::debug!(
            "Would execute MySQL query on {}:{}/{}: {}",
            conn_model.host, 
            conn_model.port, 
            conn_model.database_name.as_deref().unwrap_or(""),
            query
        );
        
        if let Some(p) = params {
            tracing::debug!("With parameters: {}", p);
        }
        
        // Mock implementation
        let columns = vec!["id".to_string(), "name".to_string(), "value".to_string()];
        let rows = vec![
            serde_json::json!({"id": 1, "name": "MySQL Item 1", "value": 100}),
            serde_json::json!({"id": 2, "name": "MySQL Item 2", "value": 200}),
        ];
        
        Ok((columns, rows))
    }
    
    async fn execute_redis_query(&self, 
        conn_model: &crate::models::database::Model,
        query: &str,
        params: Option<&serde_json::Value>
    ) -> Result<(Vec<String>, Vec<serde_json::Value>), AppError> {
        // Redis doesn't support SQL queries, so we'd interpret the query differently
        // For example, "GET mykey" would execute the GET command for "mykey"
        tracing::debug!(
            "Would execute Redis command on {}:{}: {}",
            conn_model.host, 
            conn_model.port,
            query
        );
        
        if let Some(p) = params {
            tracing::debug!("With parameters: {}", p);
        }
        
        // Mock implementation
        let columns = vec!["key".to_string(), "value".to_string(), "type".to_string()];
        let rows = vec![
            serde_json::json!({"key": "test:1", "value": "value1", "type": "string"}),
            serde_json::json!({"key": "test:2", "value": "[1,2,3]", "type": "list"}),
        ];
        
        Ok((columns, rows))
    }
    
    async fn execute_opensearch_query(&self,
        conn_model: &crate::models::database::Model,
        query: &str,
        params: Option<&serde_json::Value>
    ) -> Result<(Vec<String>, Vec<serde_json::Value>), AppError> {
        // OpenSearch uses JSON query DSL
        tracing::debug!(
            "Would execute OpenSearch query on {}:{}/{}: {}",
            conn_model.host, 
            conn_model.port,
            conn_model.database_name.as_deref().unwrap_or("default"),
            query
        );
        
        if let Some(p) = params {
            tracing::debug!("With parameters: {}", p);
        }
        
        // Mock implementation
        let columns = vec!["_id".to_string(), "_source".to_string(), "_score".to_string()];
        let rows = vec![
            serde_json::json!({"_id": "doc1", "_source": {"title": "Document 1", "content": "Sample content"}, "_score": 1.0}),
            serde_json::json!({"_id": "doc2", "_source": {"title": "Document 2", "content": "Another example"}, "_score": 0.8}),
        ];
        
        Ok((columns, rows))
    }

    // Test connection to a database
    pub async fn test_connection(&self, conn_model: &crate::models::database::Model) -> Result<ConnectionTestResult, AppError> {
        // Perform a lightweight query to test the connection
        let start_time = Utc::now();
        
        let result = match conn_model.connection_type.as_str() {
            "postgres" => {
                // For PostgreSQL, we'd execute a simple "SELECT 1" query
                let _ = self.execute_postgres_query(conn_model, "SELECT 1", None).await?;
                
                // Get server version
                let (_, rows) = self.execute_postgres_query(conn_model, "SELECT version()", None).await?;
                let version = if !rows.is_empty() {
                    rows[0].get("version").and_then(|v| v.as_str()).map(|s| s.to_string())
                        .or_else(|| Some("Unknown version".to_string()))
                } else {
                    Some("No version info".to_string())
                };
                
                ConnectionStats {
                    max_connections: 100, // Would get from pg_settings
                    current_connections: 1,
                    ssl_in_use: conn_model.ssl_mode.as_deref().unwrap_or("disable") != "disable",
                    server_encoding: "UTF8".to_string(),
                    server_version: version.unwrap_or_else(|| "Unknown".to_string()),
                }
            },
            "mysql" => {
                // Similar implementation for MySQL
                ConnectionStats {
                    max_connections: 100,
                    current_connections: 1,
                    ssl_in_use: false,
                    server_encoding: "UTF8".to_string(),
                    server_version: "MySQL Mock Version".to_string(),
                }
            },
            "redis" => {
                // For Redis
                ConnectionStats {
                    max_connections: 10000,
                    current_connections: 1,
                    ssl_in_use: false,
                    server_encoding: "UTF8".to_string(),
                    server_version: "Redis Mock Version".to_string(),
                }
            },
            "opensearch" => {
                // For OpenSearch
                ConnectionStats {
                    max_connections: 0, // Not applicable
                    current_connections: 1,
                    ssl_in_use: true,
                    server_encoding: "UTF8".to_string(),
                    server_version: "OpenSearch Mock Version".to_string(),
                }
            },
            _ => return Err(AppError::BadRequest(format!("Unsupported database type: {}", conn_model.connection_type)))
        };
        
        let latency = (Utc::now() - start_time).num_milliseconds() as u64;
        
        Ok(ConnectionTestResult {
            success: true,
            message: format!("Successfully connected to {} database", conn_model.connection_type),
            latency_ms: Some(latency),
            version_info: Some(result.server_version.clone()),
            connection_stats: Some(result),
        })
    }
    
    // Get database schema information
    pub async fn get_schema(&self, conn_model: &crate::models::database::Model) -> Result<serde_json::Value, AppError> {
        match conn_model.connection_type.as_str() {
            "postgres" => {
                // In a real implementation, you would query the PostgreSQL information_schema
                // Here we're returning mock schema data
                Ok(serde_json::json!({
                    "tables": [
                        {
                            "name": "users",
                            "schema": "public",
                            "columns": [
                                {"name": "id", "type": "uuid", "nullable": false},
                                {"name": "username", "type": "varchar", "nullable": false},
                                {"name": "email", "type": "varchar", "nullable": false},
                                {"name": "created_at", "type": "timestamp", "nullable": false}
                            ],
                            "primary_key": ["id"],
                            "foreign_keys": []
                        },
                        {
                            "name": "posts",
                            "schema": "public",
                            "columns": [
                                {"name": "id", "type": "uuid", "nullable": false},
                                {"name": "user_id", "type": "uuid", "nullable": false},
                                {"name": "title", "type": "varchar", "nullable": false},
                                {"name": "content", "type": "text", "nullable": true},
                                {"name": "created_at", "type": "timestamp", "nullable": false}
                            ],
                            "primary_key": ["id"],
                            "foreign_keys": [
                                {
                                    "columns": ["user_id"],
                                    "references_table": "users",
                                    "references_columns": ["id"]
                                }
                            ]
                        }
                    ],
                    "views": [],
                    "functions": []
                }))
            },
            "mysql" => {
                // Similar implementation for MySQL
                Ok(serde_json::json!({
                    "tables": [
                        {
                            "name": "customers",
                            "columns": [
                                {"name": "id", "type": "int", "nullable": false},
                                {"name": "name", "type": "varchar(100)", "nullable": false},
                                {"name": "email", "type": "varchar(100)", "nullable": false}
                            ],
                            "primary_key": ["id"]
                        }
                    ]
                }))
            },
            "redis" => {
                // Redis doesn't have a schema in the traditional sense
                // Return key patterns or database stats instead
                Ok(serde_json::json!({
                    "databases": 16,
                    "key_patterns": ["user:*", "session:*", "cache:*"]
                }))
            },
            "opensearch" => {
                // For OpenSearch, return index mappings
                Ok(serde_json::json!({
                    "indices": [
                        {
                            "name": "products",
                            "mappings": {
                                "properties": {
                                    "name": {"type": "text"},
                                    "description": {"type": "text"},
                                    "price": {"type": "float"},
                                    "created_at": {"type": "date"}
                                }
                            }
                        }
                    ]
                }))
            },
            _ => Err(AppError::BadRequest(format!("Unsupported database type: {}", conn_model.connection_type)))
        }
    }

    // This is a convenience method to analyze a database based on a connection model
    pub async fn analyze_connection(&self, conn_model: &crate::models::database::Model) -> Result<DatabaseAnalysis, AppError> {
        // In a real implementation, you would establish a connection to the specified database
        // and then analyze that connection. For now, we'll return a mock analysis.
        
        tracing::info!("Analyzing connection: {}", conn_model.name);
        
        // Create mock analysis data
        let analysis = DatabaseAnalysis {
            issues: vec![
                DatabaseIssue {
                    severity: IssueSeverity::Medium,
                    category: IssueCategory::Performance,
                    title: "High number of sequential scans".to_string(),
                    description: "Tables are frequently scanned sequentially rather than using indexes".to_string(),
                    recommendation: "Add indexes to frequently queried columns".to_string(),
                    affected_objects: vec!["users".to_string(), "orders".to_string()],
                },
                DatabaseIssue {
                    severity: IssueSeverity::Low,
                    category: IssueCategory::Storage,
                    title: "Unused indexes detected".to_string(),
                    description: "Several indexes are not being used by any queries".to_string(),
                    recommendation: "Consider removing unused indexes to save storage and improve write performance".to_string(),
                    affected_objects: vec!["idx_user_temp".to_string(), "idx_order_archive".to_string()],
                }
            ],
            query_stats: QueryStatistics {
                total_queries: 1250,
                slow_queries: 15,
                avg_query_time_ms: 45.3,
                top_slow_queries: vec![
                    SlowQuery {
                        query: "SELECT * FROM orders WHERE created_at > ?".to_string(),
                        avg_execution_time_ms: 1250.0,
                        execution_count: 120,
                        last_execution: Utc::now(),
                        query_plan: None,
                    }
                ],
                frequent_queries: vec![
                    FrequentQuery {
                        query: "SELECT id, name FROM users WHERE status = 'active'".to_string(),
                        execution_count: 5000,
                        avg_execution_time_ms: 12.5,
                        total_time_ms: 62500.0,
                    }
                ],
            },
            performance_metrics: PerformanceMetrics {
                connection_count: 10,
                active_sessions: 3,
                idle_sessions: 7,
                buffer_hit_ratio: 0.95,
                cache_hit_ratio: 0.85,
                deadlocks: 0,
                blocked_queries: 0,
                table_stats: vec![
                    TableStats {
                        name: "users".to_string(),
                        size_bytes: 1024 * 1024 * 5, // 5MB
                        total_rows: 10000,
                        sequential_scans: 25,
                        index_scans: 500,
                        live_row_count: 9500,
                        dead_row_count: 500,
                        last_vacuum: Some(Utc::now() - chrono::Duration::days(1)),
                        last_analyze: Some(Utc::now() - chrono::Duration::days(1)),
                    }
                ],
                index_stats: vec![
                    IndexStats {
                        name: "idx_user_id".to_string(),
                        table_name: "users".to_string(),
                        size_bytes: 1024 * 512,  // 512KB
                        is_unique: true,
                        is_primary: true,
                        index_scans: 450,
                        rows_fetched: 900,
                        unused_since: None,
                    }
                ],
            },
            cost_analysis: CostAnalysis {
                storage_cost: ResourceCost {
                    current_usage: 1024.0 * 1024.0 * 50.0, // 50MB
                    unit: "GB".to_string(),
                    cost_per_unit: 0.10,
                    total_cost: 0.5,
                    trending: TrendDirection::Stable,
                },
                compute_cost: ResourceCost {
                    current_usage: 720.0, // CPU hours
                    unit: "vCPU hours".to_string(),
                    cost_per_unit: 0.05,
                    total_cost: 36.0,
                    trending: TrendDirection::Increasing,
                },
                network_cost: ResourceCost {
                    current_usage: 10.0, // GB
                    unit: "GB".to_string(),
                    cost_per_unit: 0.09,
                    total_cost: 0.9,
                    trending: TrendDirection::Decreasing,
                },
                backup_cost: ResourceCost {
                    current_usage: 1024.0 * 1024.0 * 100.0, // 100MB
                    unit: "GB".to_string(),
                    cost_per_unit: 0.05,
                    total_cost: 5.0,
                    trending: TrendDirection::Stable,
                },
                total_monthly_cost: 42.4,
                cost_recommendations: vec![
                    CostRecommendation {
                        title: "Downsize database instance".to_string(),
                        description: "Current instance is oversized for the workload".to_string(),
                        estimated_savings: 15.0,
                        implementation_effort: "Medium".to_string(),
                        priority: "High".to_string(),
                    }
                ],
            },
        };
        
        Ok(analysis)
    }
}